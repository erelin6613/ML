#import requests
import os
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Embedding
from keras.layers import Flatten, SimpleRNN, Reshape, Input, concatenate
#import gensim.downloader as api
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np
from keras.models import Model
from keras.models import load_model
from keras import regularizers
#import time

#train_data = pd.read_csv('train.csv', encoding='utf-8')
#final_test_data = pd.read_csv('test.csv', encoding='utf-8')

def cat_to_numeric(category):
    if category=='LIFE_ARTS':
        return 1
    if category=='CULTURE':
        return 2
    if category=='SCIENCE':
        return 3
    if category=='STACKOVERFLOW':
      return 4
    if category=='TECHNOLOGY':
      return 5

def prepare_data(frame):
    to_drop = ['question_user_page', 'host', 'url', 'answer_user_page', 
               'answer_user_name', 'question_user_name', 'question_user_page']
    data = frame.drop(to_drop, axis=1)
    data['categoty_normalized'] = data['category'].apply(cat_to_numeric)
    data.drop('category', axis=1, inplace=True)
    return data

def get_vars_and_targets(train_data, test_data):
    
    train_cols = test_data.columns
    target_cols = set(train_data.columns).difference(set(test_data.columns))
    train_cols = set(train_data.columns) - target_cols
    target_cols = list(target_cols)
    train_cols = list(train_cols)
    return train_cols, target_cols

def transform_texts(frame):
    
    text_cols = ['question_title', 'question_body', 'answer']
    tokenizer = Tokenizer()
    for col in text_cols:
        tokenizer.fit_on_texts(frame[col])
    for col in text_cols:
        frame[col+'_tokenized'] = tokenizer.texts_to_sequences(frame[col])
        frame.drop([col], inplace=True, axis=1)
    return [frame, tokenizer]

def pad_texts(frame, cols, maxlen=1000):
    
    frame_padded = pd.DataFrame()
    for index in frame.index:
        padded_val = {'index': index}
        for col in cols:
            if len(frame.loc[index, col]) < maxlen:
                elem = frame.loc[index, col]
                while len(elem) < maxlen:
                    elem.append(0)
            else:
                elem = frame.loc[index, col][0:maxlen]
            padded_val[col+'_padded'] = elem
        frame_padded = frame_padded.append(padded_val, ignore_index=True)
    return frame_padded

def convert_to_arrays(frame):
    
    arrays = []
    for index in frame.index:
        merged = []
        for each in ['question_title_tokenized_padded', 'question_body_tokenized_padded', 'answer_tokenized_padded']:
            arr = np.array(frame.loc[index, each])
            #print(arr)
            merged.append(arr)
            #print(merged)
        arrays.append(np.array(merged))
    #print(np.array(X_train_new).shape)
    return np.array(arrays)
"""

def build_model(tokenizer, maxlen=750):
    input_qt = Input(shape=(maxlen, ), name='question_title_input')
    emb_qt = Embedding(len(tokenizer.word_index)+1, output_dim=512)(input_qt)
    lstm_qt = LSTM(512, return_sequences=True, recurrent_dropout=0.1)(emb_qt)
    lstm_qt2 = LSTM(512)(lstm_qt)
    
    input_qb = Input(shape=(maxlen, ), name='question_body_input')
    emb_qb = Embedding(len(tokenizer.word_index)+1, output_dim=512)(input_qb)
    lstm_qb = LSTM(512, return_sequences=True, recurrent_dropout=0.1)(emb_qb)
    lstm_qb2 = LSTM(512)(lstm_qb)
    
    input_ans = Input(shape=(maxlen, ), name='answer_input')
    emb_ans = Embedding(len(tokenizer.word_index)+1, output_dim=512)(input_ans)
    lstm_ans = LSTM(512, return_sequences=True, recurrent_dropout=0.1)(emb_ans)
    lstm_ans2 = LSTM(512)(lstm_ans)
    
    merged = concatenate([lstm_qt2, lstm_qb2, lstm_ans2])
    dense = Dense(128, activation='relu')(merged)
    dropout = Dropout(0.2)(dense)
    dense2 = Dense(64, activation='relu')(dropout)
    output = Dense(30, activation='relu', name='outputs')(dense2)
    
    model = Model(inputs=[input_qt, input_qb, input_ans], outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
    
    return model
"""

def build_model_2(tokenizer, maxlen=1000):
    input_qt = Input(shape=(3, maxlen))
    #emb_qt = Embedding(len(tokenizer.word_index)+1, output_dim=256)(input_qt)
    #flatten = Flatten()(emb_qt)   bias_regularizer=
    lstm_qt1 = LSTM(1024, return_sequences=True, dropout=0.2, activation='sigmoid')(input_qt)
    lstm_qt2 = LSTM(512, return_sequences=True, dropout=0.1, activation='sigmoid')(lstm_qt1)
    lstm_qt3 = LSTM(512, activation='sigmoid')(lstm_qt2)
    
    dense = Dense(512, activation='relu')(lstm_qt3)
    dropout = Dropout(0.2)(dense)
    dense2 = Dense(256, activation='relu')(dropout)
    output = Dense(30, name='outputs')(dense2)
    
    model = Model(inputs=[input_qt], outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
    
    return model

def predict_and_write(model, test_data):
    pass
    
    #predictions = model.predict(test_data)


def main():
    train_data = pd.read_csv('../input/google-quest-challenge/train.csv', encoding='utf-8')
    test_data = pd.read_csv('../input/google-quest-challenge/test.csv', encoding='utf-8')
    train_data = prepare_data(train_data)
    test_data = prepare_data(test_data)
    
    train_cols, target_cols = get_vars_and_targets(train_data, test_data)
    X_train, X_test, y_train, y_test = train_test_split(train_data.loc[:, train_cols], train_data.loc[:, target_cols], test_size=0.25)
    X_train, tokenizer = transform_texts(X_train)
    X_test, _ = transform_texts(X_test)
    
    X_train = pad_texts(X_train, cols=['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized'])
    X_test = pad_texts(X_test, cols=['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized'])
    #print(y_train.columns)
    #print(len(test_data.columns))
    X_train = convert_to_arrays(X_train)
    X_test = convert_to_arrays(X_test)
    model = build_model_2(tokenizer)
    model.fit(X_train, y_train, batch_size=16, epochs=100, validation_split=0.2)

    #model.fit({'question_title_input': np.array(X_train['question_title_tokenized_padded'].tolist()),
    #           'question_body_input': np.array(X_train['question_body_tokenized_padded'].tolist()),
    #           'answer_input': np.array(X_train['answer_tokenized_padded'].tolist())},
    #          {'outputs': np.array(y_train.as_matrix())}, batch_size=16, epochs=1, validation_split=0.2)
    model.save('main_model.model')
    
    print(model.evaluate(X_test, y_test))
    
    #print(test_data.columns)
    test_data, _ = transform_texts(test_data.drop(['qa_id', 'categoty_normalized'], axis=1))
    test_data = pad_texts(test_data, cols=['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized'])
    test_data = convert_to_arrays(test_data)
    #with open('predictions.txt', 'w') as file: 
    #    file.write(model.predict(test_data))
    #    print(type(model.predict(test_data)))
    print(test_data)



main()

