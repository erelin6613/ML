import os
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Embedding, Input
from keras.layers import Flatten, SimpleRNN, Reshape, RNN
from keras.layers import BatchNormalization, ConvLSTM2D
from keras.layers import SimpleRNNCell, concatenate, Add
from keras import regularizers
#import gensim.downloader as api
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np
from keras.models import Model
from keras.initializers import RandomNormal
from keras.models import load_model
from keras import regularizers
from keras.initializers import RandomUniform
import time
from tqdm import tqdm

start_time = time.time()
#train_data = pd.read_csv('train.csv', encoding='utf-8')
#final_test_data = pd.read_csv('test.csv', encoding='utf-8')

def cat_to_numeric(category):
    if category=='LIFE_ARTS':
        return 1
    if category=='CULTURE':
        return 2
    if category=='SCIENCE':
        return 3
    if category=='STACKOVERFLOW':
      return 4
    if category=='TECHNOLOGY':
      return 5

def prepare_data(frame):
    to_drop = ['question_user_page', 'host', 'url', 'answer_user_page', 
               'answer_user_name', 'question_user_name', 'question_user_page']
    data = frame.drop(to_drop, axis=1)
    data['categoty_normalized'] = data['category'].apply(cat_to_numeric)
    data.drop('category', axis=1, inplace=True)
    return data

def get_vars_and_targets(train_data, test_data):
    
    train_cols = test_data.columns
    target_cols = set(train_data.columns).difference(set(test_data.columns))
    train_cols = set(train_data.columns) - target_cols
    target_cols = list(target_cols)
    train_cols = list(train_cols)
    return train_cols, target_cols

def transform_texts(frame):
    
    text_cols = ['question_title', 'question_body', 'answer']
    tokenizer = Tokenizer()
    for col in text_cols:
        tokenizer.fit_on_texts(frame[col])
    for col in text_cols:
        frame[col+'_tokenized'] = tokenizer.texts_to_sequences(frame[col])
        frame.drop([col], inplace=True, axis=1)
    return [frame, tokenizer]

def pad_texts(frame, cols, maxlen=1000):
    
    frame_padded = pd.DataFrame()
    for index in frame.index:
        padded_val = {'index': index}
        for col in cols:
            if len(frame.loc[index, col]) < maxlen:
                elem = frame.loc[index, col]
                while len(elem) < maxlen:
                    elem.append(0)
            else:
                elem = frame.loc[index, col][0:maxlen]
            padded_val[col+'_padded'] = elem
        frame_padded = frame_padded.append(padded_val, ignore_index=True)
    return frame_padded

def convert_to_arrays(frame):
    
    arrays = []
    for index in frame.index:
        merged = []
        for each in ['question_title_tokenized_padded', 'question_body_tokenized_padded', 'answer_tokenized_padded']:
            arr = np.array(frame.loc[index, each])
            #print(arr)
            merged.append(arr)
            #print(merged)
        arrays.append(np.array(merged))
    #print(np.array(X_train_new).shape)
    return np.array(arrays)

def model_4(tokenizer, maxlen=750):

    input_qt = Input(shape=(1, maxlen))
    emb_qt = Embedding(len(tokenizer.word_index)+1, output_dim=3)(input_qt)
    reshape_qt = Reshape((1, maxlen))(emb_qt)
    lstm_qt = RNN(SimpleRNNCell(1024, recurrent_dropout=0.1, activation='sigmoid', 
        recurrent_regularizer=regularizers.l1(0.01)), return_sequences=True)(reshape_qt)
    lstm_qt2 = RNN(SimpleRNNCell(512, activation='sigmoid', recurrent_regularizer=regularizers.l1(0.01)))(lstm_qt)
    dense_qt = Dense(256, activation='relu')(lstm_qt2)

    input_qb = Input(shape=(1, maxlen))
    emb_qb = Embedding(len(tokenizer.word_index)+1, output_dim=3)(input_qb)
    reshape_qb = Reshape((1, maxlen))(emb_qb)
    lstm_qb = RNN(SimpleRNNCell(1024, recurrent_dropout=0.1, activation='sigmoid', 
        recurrent_regularizer=regularizers.l1(0.01)), return_sequences=True)(reshape_qb)
    lstm_qb2 = RNN(SimpleRNNCell(512, activation='sigmoid', recurrent_regularizer=regularizers.l1(0.01)))(lstm_qb)
    dense_qb = Dense(256, activation='relu')(lstm_qb2)

    input_ans = Input(shape=(1, maxlen))
    emb_ans = Embedding(len(tokenizer.word_index)+1, output_dim=3)(input_ans)
    reshape_ans = Reshape((1, maxlen))(emb_ans)
    lstm_ans = RNN(SimpleRNNCell(1024, recurrent_dropout=0.1, activation='sigmoid', 
        recurrent_regularizer=regularizers.l1(0.01)), return_sequences=True)(reshape_ans)
    lstm_ans2 = RNN(SimpleRNNCell(512, activation='sigmoid', recurrent_regularizer=regularizers.l1(0.01)))(lstm_ans)
    dense_ans = Dense(256, activation='relu')(lstm_ans2)

    merged_dense = Add()([dense_qt, dense_qb, dense_ans])
    droput = Dropout(0.2)(merged_dense)
    dense_1 = Dense(128, activation='relu')(droput)
    dense_2 = Dense(64, activation='relu')(dense_1)
    output = Dense(30)(dense_2)

    model = Model(inputs=[input_qt, input_qb, input_ans], outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])

    return model

def model_5_rnn(tokenizer, maxlen=200):
    
    initializer = RandomUniform(seed=66)

    input_qt = Input(shape=(maxlen, ))
    emb_qt = Embedding(len(tokenizer.word_index)+1, output_dim=1024)(input_qt)
    #reshape_qt = Reshape((1, maxlen))(emb_qt)
    lstm_qt = RNN(SimpleRNNCell(512, recurrent_dropout=0.1, activation='sigmoid', bias_initializer=initializer), return_sequences=True)(emb_qt)
    lstm_qt2 = RNN(SimpleRNNCell(256, recurrent_dropout=0.1, activation='sigmoid', bias_initializer=initializer))(lstm_qt)
    dense_qt = Dense(128, activation='relu')(lstm_qt2)

    input_qb = Input(shape=(maxlen, ))
    emb_qb = Embedding(len(tokenizer.word_index)+1, output_dim=1024)(input_qb)
    #reshape_qb = Reshape((1, maxlen))(emb_qb)
    lstm_qb = RNN(SimpleRNNCell(512, recurrent_dropout=0.1, activation='sigmoid', bias_initializer=initializer), return_sequences=True)(emb_qb)
    lstm_qb2 = RNN(SimpleRNNCell(256, recurrent_dropout=0.1, activation='sigmoid', bias_initializer=initializer))(lstm_qb)
    dense_qb = Dense(128, activation='relu')(lstm_qb2)

    input_ans = Input(shape=(maxlen, ))
    emb_ans = Embedding(len(tokenizer.word_index)+1, output_dim=1024)(input_ans)
    #reshape_ans = Reshape((1, maxlen))(emb_ans)
    lstm_ans = RNN(SimpleRNNCell(512, recurrent_dropout=0.1, activation='sigmoid', bias_initializer=initializer), return_sequences=True)(emb_ans)
    lstm_ans2 = RNN(SimpleRNNCell(256, recurrent_dropout=0.1, activation='sigmoid', bias_initializer=initializer))(lstm_ans)
    dense_ans = Dense(128, activation='relu')(lstm_ans2)

    merged_dense = Add()([dense_qt, dense_qb, dense_ans])
    droput = Dropout(0.2)(merged_dense)
    dense_1 = Dense(64, activation='relu')(droput)
    dense_2 = Dense(32, activation='relu')(dense_1)
    output = Dense(30)(dense_2)

    model = Model(inputs=[input_qt, input_qb, input_ans], outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])

    return model





def main():
    train_data = pd.read_csv('../input/google-quest-challenge/train.csv', encoding='utf-8')
    test_data = pd.read_csv('../input/google-quest-challenge/test.csv', encoding='utf-8')

    train_data = prepare_data(train_data)
    test_data = prepare_data(test_data)
    
    train_cols, target_cols = get_vars_and_targets(train_data, test_data)
    X_train, X_test, y_train, y_test = train_test_split(train_data.loc[:, train_cols], train_data.loc[:, target_cols], test_size=0.25)
    X_train, tokenizer = transform_texts(X_train)
    X_test, _ = transform_texts(X_test)
    X_train_qt = pad_sequences(X_train['question_title_tokenized'], maxlen=200, padding='pre')
    X_train_qb = pad_sequences(X_train['question_body_tokenized'], maxlen=200, padding='pre')
    X_train_ans = pad_sequences(X_train['answer_tokenized'], maxlen=200, padding='pre')
    X_test_qt = pad_sequences(X_test['question_title_tokenized'], maxlen=200, padding='pre')
    X_test_qb = pad_sequences(X_test['question_body_tokenized'], maxlen=200, padding='pre')
    X_test_ans = pad_sequences(X_test['answer_tokenized'], maxlen=200, padding='pre')
    #X_train = pad_texts(X_train, cols=['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized'], expand=True)
    #X_test = pad_texts(X_test, cols=['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized'], expand=True)
    #print(np.array([[y] for y in y_train.as_matrix()]))
    #print(X_train['question_title_tokenized'].str.len().mean()) # 9.218
    #print(X_train['question_body_tokenized'].str.len().mean()) # 138.459
    #print(X_train['answer_tokenized'].str.len().mean()) # 143.221

    model = model_5_rnn(tokenizer)
    
    #test_data, _ = transform_texts(test_data.drop(['qa_id', 'categoty_normalized'], axis=1))
    #test_data = convert_to_arrays(test_data)
    #predictions = model.predict(test_data)
    

    model.fit([X_train_qt, X_train_qb, X_train_ans], y_train.as_matrix(), batch_size=32, epochs=10, validation_split=0.2)

    #model.save('model_multiple_inp_out_lstm.model')
    
    print(model.evaluate([X_test_qt, X_test_qb, X_test_ans], y_test.as_matrix()))
    
    test_data, _ = transform_texts(test_data.drop(['qa_id', 'categoty_normalized'], axis=1))
    test_data_qt = pad_sequences(test_data['question_title_tokenized'], maxlen=200, padding='pre')
    test_data_qb = pad_sequences(test_data['question_body_tokenized'], maxlen=200, padding='pre')
    test_data_ans = pad_sequences(test_data['answer_tokenized'], maxlen=200, padding='pre')
    
    
    #test_data = convert_to_arrays(test_data)
    predictions = model.predict([test_data_qt, test_data_qb, test_data_ans])

        
    submission_data = pd.read_csv('../input/google-quest-challenge/sample_submission.csv', encoding='utf-8')

    print(submission_data)

    submissions = pd.DataFrame()
    print(len(submission_data.columns))
    print(submission_data.columns)
    
    row = 0
    for each in submission_data['qa_id']:
        col = 0
        submissions.loc[each, 'qa_id'] = str(each)
        for column in submission_data.drop(['qa_id'], axis=1):
        #for column in target_cols:
            if round(abs(predictions[row, col]), 5) < 0.001:
                submissions.loc[each, column] = float(0)
            else:
                submissions.loc[each, column] = round(abs(predictions[row, col]), 5)
            col+=1
        row+=1

    submissions['qa_id'] = submissions['qa_id'].astype(object)
    
    # Thanks JIANJIAN for sharing https://www.kaggle.com/buaazijian/csv-of-qa/notebook?scriptVersionId=26651714
    
    pred_df = submissions

    sample_csv = "../input/google-quest-challenge/sample_submission.csv"
    sample_df = pd.read_csv(sample_csv)
    n=0
    for line in tqdm(pred_df.values):
        for i in range(len(sample_df)):
            if(sample_df.loc[i]['qa_id']==int(line[0])):
                for j in range(1,31):
                    sample_df.iloc[i,j] = line[j]
                break
    sample_df.head()
    sample_df.to_csv("submission.csv", index=False)
    print("done!")
    print(sample_df.head())
    
    
    

main()
print((time.time() - start_time)/60, ' minutes')
