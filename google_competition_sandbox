import os
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Embedding, Input
from keras.layers import SimpleRNN, RNN
from keras.layers import BatchNormalization, ConvLSTM2D
from keras.layers import SimpleRNNCell, concatenate, Add
from keras import regularizers
#import gensim.downloader as api
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np
from keras.models import Model
from keras.initializers import RandomNormal
from keras.models import load_model
from keras import regularizers
from keras.initializers import RandomUniform
import time
from tqdm import tqdm

start_time = time.time()
#train_data = pd.read_csv('train.csv', encoding='utf-8')
#final_test_data = pd.read_csv('test.csv', encoding='utf-8')

def cat_to_numeric(category):
    if category=='LIFE_ARTS':
        return 1
    if category=='CULTURE':
        return 2
    if category=='SCIENCE':
        return 3
    if category=='STACKOVERFLOW':
        return 4
    if category=='TECHNOLOGY':
        return 5

def prepare_data(frame):
    
    to_drop = []
    for col in frame.columns:
        if 'user_page' in col or 'host' in col or\
        'url' in col or 'user_name' in col or 'categ' in col:
            to_drop.append(col)

    data = frame.drop(to_drop, axis=1)
    #data['categoty_normalized'] = data['category'].apply(cat_to_numeric)
    return data


def get_vars_and_targets(train_data, test_data):
    
    train_cols = test_data.columns
    target_cols = set(train_data.columns).difference(set(test_data.columns))
    train_cols = set(train_data.columns) - target_cols
    target_cols = list(target_cols)
    train_cols = list(train_cols)
    return train_cols, target_cols

def get_text_cols(frame):
    
    text_cols = []
    for col in frame.columns:
        if 'title' in col or 'body' in col or col=='answer':
            text_cols.append(col)
            
    return text_cols


def transform_texts(frame):
    
    #text_cols = ['question_title', 'question_body', 'answer']
    text_cols = get_text_cols(frame)
    tokenizer = Tokenizer()
    for col in text_cols:
        tokenizer.fit_on_texts(frame[col])
    renamed_cols = []
    for col in text_cols:
        renamed_cols.append(col+'_tokenized')
        frame[col+'_tokenized'] = tokenizer.texts_to_sequences(frame[col])
        frame.drop([col], inplace=True, axis=1)
    return [frame, renamed_cols, tokenizer]

def pad_texts(frame, cols, maxlen=1000):
    
    frame_padded = pd.DataFrame()
    for index in frame.index:
        padded_val = {'index': index}
        for col in cols:
            if len(frame.loc[index, col]) < maxlen:
                elem = frame.loc[index, col]
                while len(elem) < maxlen:
                    elem.append(0)
            else:
                elem = frame.loc[index, col][0:maxlen]
            padded_val[col+'_padded'] = elem
        frame_padded = frame_padded.append(padded_val, ignore_index=True)
    return frame_padded

def convert_to_arrays(frame):
    
    arrays = []
    for index in frame.index:
        merged = []
        for each in ['question_title_tokenized_padded', 'question_body_tokenized_padded', 'answer_tokenized_padded']:
            arr = np.array(frame.loc[index, each])
            #print(arr)
            merged.append(arr)
            #print(merged)
        arrays.append(np.array(merged))
    #print(np.array(X_train_new).shape)
    return np.array(arrays)

def model_4(tokenizer, maxlen=750):

    input_qt = Input(shape=(1, maxlen))
    emb_qt = Embedding(len(tokenizer.word_index)+1, output_dim=3)(input_qt)
    reshape_qt = Reshape((1, maxlen))(emb_qt)
    lstm_qt = RNN(SimpleRNNCell(1024, recurrent_dropout=0.1, activation='sigmoid', 
        recurrent_regularizer=regularizers.l1(0.01)), return_sequences=True)(reshape_qt)
    lstm_qt2 = RNN(SimpleRNNCell(512, activation='sigmoid', recurrent_regularizer=regularizers.l1(0.01)))(lstm_qt)
    dense_qt = Dense(256, activation='relu')(lstm_qt2)

    input_qb = Input(shape=(1, maxlen))
    emb_qb = Embedding(len(tokenizer.word_index)+1, output_dim=3)(input_qb)
    reshape_qb = Reshape((1, maxlen))(emb_qb)
    lstm_qb = RNN(SimpleRNNCell(1024, recurrent_dropout=0.1, activation='sigmoid', 
        recurrent_regularizer=regularizers.l1(0.01)), return_sequences=True)(reshape_qb)
    lstm_qb2 = RNN(SimpleRNNCell(512, activation='sigmoid', recurrent_regularizer=regularizers.l1(0.01)))(lstm_qb)
    dense_qb = Dense(256, activation='relu')(lstm_qb2)

    input_ans = Input(shape=(1, maxlen))
    emb_ans = Embedding(len(tokenizer.word_index)+1, output_dim=3)(input_ans)
    reshape_ans = Reshape((1, maxlen))(emb_ans)
    lstm_ans = RNN(SimpleRNNCell(1024, recurrent_dropout=0.1, activation='sigmoid', 
        recurrent_regularizer=regularizers.l1(0.01)), return_sequences=True)(reshape_ans)
    lstm_ans2 = RNN(SimpleRNNCell(512, activation='sigmoid', recurrent_regularizer=regularizers.l1(0.01)))(lstm_ans)
    dense_ans = Dense(256, activation='relu')(lstm_ans2)

    merged_dense = Add()([dense_qt, dense_qb, dense_ans])
    droput = Dropout(0.2)(merged_dense)
    dense_1 = Dense(128, activation='relu')(droput)
    dense_2 = Dense(64, activation='relu')(dense_1)
    output = Dense(30)(dense_2)

    model = Model(inputs=[input_qt, input_qb, input_ans], outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
    print(model.summary())

    return model

def model_5_rnn(X_train_padded, tokenizer, maxlen=200):
    
    initializer = RandomUniform(seed=69)
    #j = 0
    inputs = []
    features = []
    for each in X_train_padded:
        input_layer = Input(shape=(maxlen, ))
        emb_layer = Embedding(len(tokenizer.word_index)+1, output_dim=1024)(input_layer)
        rnn_layer_1 = RNN(SimpleRNNCell(512, recurrent_dropout=0.1, activation='sigmoid', bias_initializer=initializer), return_sequences=True)(emb_layer)
        rnn_layer_2 = RNN(SimpleRNNCell(256, activation='sigmoid', bias_initializer=initializer))(rnn_layer_1)
        dense_layer = Dense(128, activation='relu')(rnn_layer_2)
        features.append(dense_layer)
        inputs.append(input_layer)

    merged_dense = Add()(features)
    droput = Dropout(0.2)(merged_dense)
    dense_1 = Dense(64, activation='relu')(droput)
    dense_2 = Dense(32, activation='relu')(dense_1)
    output = Dense(30)(dense_2)

    model = Model(inputs=inputs, outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
    print(model.summary())

    return model





def main():
    train_data = pd.read_csv('../input/google-quest-challenge/train.csv')
    test_data = pd.read_csv('../input/google-quest-challenge/test.csv')

    train_data = prepare_data(train_data)
    test_data = prepare_data(test_data)
    
    train_cols, target_cols = get_vars_and_targets(train_data, test_data)
    X_train, X_test, y_train, y_test = train_test_split(train_data.loc[:, train_cols], train_data.loc[:, target_cols], test_size=0.25)
    X_train, text_cols, tokenizer = transform_texts(X_train)
    X_test, text_cols, _ = transform_texts(X_test)
    X_train_features = []
    X_test_features = []
    for col in text_cols:
        X_train_features.append('X_train_'+(('_').join(col.split('_')[:-1])))
        X_test_features.append('X_test_'+(('_').join(col.split('_')[:-1])))
    X_train_padded = []
    X_test_padded = []
    i = 0
    for each in X_train_features:
        X_train_padded.append(pad_sequences(X_train[text_cols[i]], maxlen=200, padding='pre'))
        X_test_padded.append(pad_sequences(X_test[text_cols[i]], maxlen=200, padding='pre'))
        i += 1
    #X_train_qt = pad_sequences(X_train['question_title_tokenized'], maxlen=200, padding='pre')
    #X_train_qb = pad_sequences(X_train['question_body_tokenized'], maxlen=200, padding='pre')
    #X_train_ans = pad_sequences(X_train['answer_tokenized'], maxlen=200, padding='pre')
    #X_test_qt = pad_sequences(X_test['question_title_tokenized'], maxlen=200, padding='pre')
    #X_test_qb = pad_sequences(X_test['question_body_tokenized'], maxlen=200, padding='pre')
    #X_test_ans = pad_sequences(X_test['answer_tokenized'], maxlen=200, padding='pre')

    model = model_5_rnn(X_train_padded, tokenizer)
    model.fit(X_train_padded, y_train.as_matrix(), batch_size=32, epochs=1, validation_split=0.2)
    print(model.evaluate(X_test_padded, y_test.as_matrix()))
    
    test_data, text_cols, _ = transform_texts(test_data.drop(['qa_id'], axis=1))
    test_features = []
    for col in text_cols:
        test_features.append('test_feature_'+(('_').join(col.split('_')[:-1])))
    test_padded = []
    i = 0
    for each in test_features:
        test_padded.append(pad_sequences(test_data[text_cols[i]], maxlen=200, padding='pre'))
        i += 1 
    
    predictions = model.predict(test_padded)
    
    
    ##########################################3

    submission_data = pd.read_csv('../input/google-quest-challenge/sample_submission.csv', encoding='utf-8')
    #submission_data = submission_data.drop(get_text_cols(submission_data), axis=1)
    labels = list(submission_data.columns[1:].values)
    submission_data[labels] = np.absolute(predictions)
    #print(submission_data)

    submissions = pd.DataFrame()
    print(len(submission_data.columns))
    print(submission_data.columns)
    submission_data.to_csv('submission.csv', index=False)
"""    
    row = 0
    for each in submission_data['qa_id']:
        col = 0
        submissions.loc[each, 'qa_id'] = str(each)
        for column in target_cols:
        #for column in target_cols:
            if round(abs(predictions[row, col]), 5) < 0.001:
                submissions.loc[each, column] = float(0)
            else:
                submissions.loc[each, column] = round(abs(predictions[row, col]), 5)
            col+=1
        row+=1
"""
    #submissions['qa_id'] = submissions['qa_id'].astype(object)
    
    

    
main()
print((time.time() - start_time)/60, ' minutes')
