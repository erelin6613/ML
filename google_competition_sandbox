#import requests
import os
import json
import pandas as pd
from sklearn.model_selection import train_test_split
from keras.preprocessing.text import Tokenizer
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout, Embedding, Input
from keras.layers import Flatten, SimpleRNN, Reshape, concatenate
from keras.layers import BatchNormalization, ConvLSTM2D, RNN, SimpleRNNCell
#import gensim.downloader as api
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
import numpy as np
from keras.models import Model
from keras.models import load_model
from keras import regularizers
#import talos
#from talos.model.normalizers import lr_normalizer
#import time

#train_data = pd.read_csv('train.csv', encoding='utf-8')
#final_test_data = pd.read_csv('test.csv', encoding='utf-8')



def cat_to_numeric(category):
    if category=='LIFE_ARTS':
        return 1
    if category=='CULTURE':
        return 2
    if category=='SCIENCE':
        return 3
    if category=='STACKOVERFLOW':
      return 4
    if category=='TECHNOLOGY':
      return 5

def prepare_data(frame):
    to_drop = ['question_user_page', 'host', 'url', 'answer_user_page', 
               'answer_user_name', 'question_user_name', 'question_user_page']
    data = frame.drop(to_drop, axis=1)
    data['categoty_normalized'] = data['category'].apply(cat_to_numeric)
    data.drop('category', axis=1, inplace=True)
    return data

def get_vars_and_targets(train_data, test_data):
    
    train_cols = test_data.columns
    target_cols = set(train_data.columns).difference(set(test_data.columns))
    train_cols = set(train_data.columns) - target_cols
    target_cols = list(target_cols)
    train_cols = list(train_cols)
    return train_cols, target_cols

def transform_texts(frame):
    
    text_cols = ['question_title', 'question_body', 'answer']
    tokenizer = Tokenizer()
    for col in text_cols:
        tokenizer.fit_on_texts(frame[col])
    for col in text_cols:
        frame[col+'_tokenized'] = tokenizer.texts_to_sequences(frame[col])
        frame.drop([col], inplace=True, axis=1)
    return [frame, tokenizer]

def pad_texts(frame, cols, maxlen=500, expand=True):
    
    frame_padded = pd.DataFrame()
    if expand:
    	for index in frame.index:
        	padded_val = {'index': index}
        	for col in cols:
        		if len(frame.loc[index, col]) < maxlen:
        			elem = frame.loc[index, col]
        			while len(elem) < maxlen:
        				elem.append(0)
        		else:
        			elem = frame.loc[index, col][0:maxlen]
        		padded_val[col+'_padded'] = elem
        	frame_padded = frame_padded.append(padded_val, ignore_index=True)
    else:
    	for index in frame.index:
    		padded_val = {'index': index}
    		for col in cols:
    			elem = frame.loc[index, col]
    			padded_val[col+'_padded'] = elem
    		frame_padded = frame_padded.append(padded_val, ignore_index=True)    

    return frame_padded

def convert_to_arrays(frame):
    
    arrays = []
    for index in frame.index:
        merged = []
        try:
        	for each in ['question_title_tokenized_padded', 'question_body_tokenized_padded', 'answer_tokenized_padded']:
        		arr = np.array(frame.loc[index, each])
        		merged.append(arr)
        	arrays.append(np.array(merged))
        except Exception:
         	for each in ['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized']:
         		arr = np.array(frame.loc[index, each])
         		merged.append(arr)
         	arrays.append(np.array(merged))

    return np.array(arrays)
"""

def build_model(tokenizer, maxlen=750):
    input_qt = Input(shape=(maxlen, ), name='question_title_input')
    emb_qt = Embedding(len(tokenizer.word_index)+1, output_dim=512)(input_qt)
    lstm_qt = LSTM(512, return_sequences=True, recurrent_dropout=0.1)(emb_qt)
    lstm_qt2 = LSTM(512)(lstm_qt)
    
    input_qb = Input(shape=(maxlen, ), name='question_body_input')
    emb_qb = Embedding(len(tokenizer.word_index)+1, output_dim=512)(input_qb)
    lstm_qb = LSTM(512, return_sequences=True, recurrent_dropout=0.1)(emb_qb)
    lstm_qb2 = LSTM(512)(lstm_qb)
    
    input_ans = Input(shape=(maxlen, ), name='answer_input')
    emb_ans = Embedding(len(tokenizer.word_index)+1, output_dim=512)(input_ans)
    lstm_ans = LSTM(512, return_sequences=True, recurrent_dropout=0.1)(emb_ans)
    lstm_ans2 = LSTM(512)(lstm_ans)
    
    merged = concatenate([lstm_qt2, lstm_qb2, lstm_ans2])
    dense = Dense(128, activation='relu')(merged)
    dropout = Dropout(0.2)(dense)
    dense2 = Dense(64, activation='relu')(dropout)
    output = Dense(30, activation='relu', name='outputs')(dense2)
    
    model = Model(inputs=[input_qt, input_qb, input_ans], outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
    
    return model
"""

def build_model_2(tokenizer, maxlen=500):
    input_qt = Input(shape=(3, maxlen))
    norm_qt = BatchNormalization()(input_qt)
    emb_qt = Embedding(len(tokenizer.word_index)+1, output_dim=5)(input_qt)
    #flatten = Flatten()(emb_qt)   bias_regularizer=
    lstm_qt1 = RNN(SimpleRNNCell(1024, dropout=0.2, activation='sigmoid'), return_sequences=True)(norm_qt)
    lstm_qt2 = RNN(SimpleRNNCell(512, dropout=0.2, activation='sigmoid'), return_sequences=True)(lstm_qt1)
    norm_qt = BatchNormalization()(lstm_qt2)
    lstm_qt3 = RNN(SimpleRNNCell(512, activation='sigmoid'))(norm_qt)
    
    dense = Dense(512, activation='relu')(lstm_qt3)
    dropout = Dropout(0.2)(dense)
    dense2 = Dense(256, activation='relu')(dropout)
    output = Dense(30, name='outputs')(dense2)
    
    model = Model(inputs=[input_qt], outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
    
    return model

def model_3(tokenizer, maxlen=500):

    input_qt = Input(shape=(3, maxlen))
    #emb_qt = Embedding(len(tokenizer.word_index)+1)(input_qt)
    norm_qt = BatchNormalization()(input_qt)

    lstm_qt1 = LSTM(1024, return_sequences=True, recurrent_dropout=0.15, activation='sigmoid')(norm_qt)
    lstm_qt2 = LSTM(512, return_sequences=True, recurrent_dropout=0.1, activation='sigmoid')(lstm_qt1)
    norm_qt = BatchNormalization()(lstm_qt2)
    lstm_qt3 = LSTM(512, activation='sigmoid')(norm_qt)
    
    dense = Dense(512, activation='relu')(lstm_qt3)
    dropout = Dropout(0.2)(dense)
    dense2 = Dense(256, activation='relu')(dropout)
    output = Dense(30, name='outputs')(dense2)
    
    model = Model(inputs=[input_qt], outputs=[output])
    model.compile(optimizer='Adam', loss='mean_squared_error', metrics=['accuracy'])
    
    return model




def main():
    #train_data = pd.read_csv('../input/google-quest-challenge/train.csv', encoding='utf-8')
    #test_data = pd.read_csv('../input/google-quest-challenge/test.csv', encoding='utf-8')
    train_data = pd.read_csv('train.csv', encoding='utf-8')
    test_data = pd.read_csv('test.csv', encoding='utf-8')

    params = {'recurrent_dropout': [0.1, 0.15, 0.20],
			'dropout': [0.1, 0.15, 0.20],
			'activation': ['tanh', 'sigmoid', None, 'hard_sigmoid'],
			'lr': [0.5, 5, 10],
			'batch_size': [4, 16, 32],
			'epochs': [100, 500, 750],
			'optimizer': ['Adam', 'Nadam', 'RMSprop'],
			'validation_split': [0.15, 0.2, 0.25]
		}

    train_data = prepare_data(train_data)
    test_data = prepare_data(test_data)
    
    train_cols, target_cols = get_vars_and_targets(train_data, test_data)
    X_train, X_test, y_train, y_test = train_test_split(train_data.loc[:, train_cols], train_data.loc[:, target_cols], test_size=0.25)
    X_train, tokenizer = transform_texts(X_train)
    X_test, _ = transform_texts(X_test)
    
    X_train = pad_texts(X_train, cols=['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized'], expand=True)
    X_test = pad_texts(X_test, cols=['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized'], expand=True)

    X_train = convert_to_arrays(X_train)
    X_test = convert_to_arrays(X_test)

    #talos.Scan(X_train, y_train, model=model_3, params=params, experiment_name='talos_stats', functional_model=True)

    model = build_model_2(tokenizer)
    model.fit(X_train, y_train, batch_size=16, epochs=10, validation_split=0.2)

    model.save('model_conv_lstm.model')
    
    print(model.evaluate(X_test, y_test))
    
    test_data, _ = transform_texts(test_data.drop(['qa_id', 'categoty_normalized'], axis=1))
    test_data = pad_texts(test_data, cols=['question_title_tokenized', 'question_body_tokenized', 'answer_tokenized'])
    test_data = convert_to_arrays(test_data)
    predictions = model.predict(test_data)
        
    #submission_data = pd.read_csv('../input/google-quest-challenge/sample_submission.csv', encoding='utf-8')
    submission_data = pd.read_csv('sample_submission.csv', encoding='utf-8')
    print(submission_data)
    submissions = pd.DataFrame()
    print(len(submission_data.columns))
    print(submission_data.columns)
    row = 0
    for each in submission_data['qa_id']:
        col = 0
        submissions.loc[each, 'qa_id'] = str(each)
        for column in submission_data.drop(['qa_id'], axis=1):
            submissions.loc[each, column] = predictions[row, col]
            col+=1
        row+=1


    submissions.to_csv('submission.csv')

main()
